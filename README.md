# ğŸ‰ **gpt-oss-120b** & **gpt-oss-20b** â€“ The *â€œJustâ€¦ 120 Billion Parametersâ€* Model Suite

![gpt-oss logo](./docs/gpt-oss.svg)

<p align="center">
  <a href="https://gpt-oss.com"><strong>Try gpt-oss (if youâ€™re feeling brave)</strong></a> Â·
  <a href="https://cookbook.openai.com/topic/gpt-oss"><strong>Guides (aka â€œHow Not to Crash Your GPUâ€)</strong></a> Â·
  <a href="https://arxiv.org/abs/2508.10925"><strong>Model Card (the 2â€‘page novella)</strong></a> Â·
  <a href="https://openai.com/index/introducing-gpt-oss/"><strong>OpenAI Blog (mostly hype)</strong></a>
</p>

<p align="center">
  <strong>
    <a href="https://huggingface.co/openai/gptoss-120b">Download gpt-ossâ€‘120b</a> |
    <a href="https://huggingface.co/openai/gpt-oss-20b">Download gpt-ossâ€‘20b</a>
  </strong>
</p>

---

## What the heck is this?

We finally decided to **openâ€‘source** something that looks impressive on paper:

| Model | â€œRealâ€ Params | â€œActiveâ€ Params | GPU Required |
|-------|---------------|-----------------|--------------|
| `gpt-oss-120b` | 117â€¯B (pretend it's 120â€¯B) | 5.1â€¯B (the rest are on a nap) | One 80â€¯GB beast (H100, MI300X, or a very patient bank loan) |
| `gpt-oss-20b`  | 21â€¯B (close enough)      | 3.6â€¯B (the rest are shy) | Anything with 16â€¯GB + a strong coffee habit |

Both were trained with the **â€œharmonyâ€** response format. If you donâ€™t speak harmony, youâ€™ll just get gibberishâ€”so **read the docs** (or donâ€™t, youâ€™ll figure it out eventually).

---

## Highlights (because we love buzzwords)

- **Apacheâ€¯2.0** â€“ you can use it, sell it, and still blame the model when things go wrong.  
- **Configurable reasoning effort** â€“ choose â€œlowâ€, â€œmediumâ€, or â€œhighâ€ (aka â€œwait longer for the same answerâ€).  
- **Full chainâ€‘ofâ€‘thought** â€“ because you love reading the modelâ€™s inner monologue while you wait.  
- **Fineâ€‘tunable** â€“ you can *fineâ€‘tune* it, if you can still afford the GPU bill.  
- **Agentic capabilities** â€“ builtâ€‘in tools for browsing, Python, and pretending to be a helpful assistant.  
- **MXFP4 quantization** â€“ a fancy way to say â€œwe squeezed this into a single GPU, but weâ€™re still not sure howâ€.

---

## Quickâ€‘andâ€‘Dirty Inference (aka â€œMake it work, kind ofâ€)

### ğŸ¤— Transformers

```python
from transformers import pipeline
import torch

pipe = pipeline(
    "text-generation",
    model="openai/gpt-oss-120b",
    torch_dtype="auto",
    device_map="auto",   # â€œautoâ€ = pray
)

msgs = [{"role": "user", "content": "Explain quantum mechanics in a tweet."}]
out = pipe(msgs, max_new_tokens=256)
print(out[0]["generated_text"])
```
Pro tip: Use the builtâ€‘in chat template or youâ€™ll have to manually format the harmony JSON yourself.

### vLLM (the â€œfastâ€ way)

```python
uv pip install --pre vllm==0.10.1+gptoss \
    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \
    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
    --index-strategy unsafe-best-match

vllm serve openai/gpt-oss-20b   # spins up an OpenAIâ€‘compatible server in ~5 minutes
```

### Ollama (because â€œlocalâ€ sounds fun)

```
ollama pull gpt-oss:20b   # trust us, itâ€™s tiny
ollama run gpt-oss:20b
```

### LMâ€¯Studio (the GUI for people who hate the terminal)

```
lms get openai/gpt-oss-20b   # click a button, pray for RAM
```

# Repo Layout (in case youâ€™re nosy)

torch/ â€“ a slow reference implementation (needs a miniâ€‘cluster of H100s).  
triton/ â€“ a slightly less slow reference that actually uses the newâ€‘fangled Triton kernels.  
metal/ â€“ for Appleâ€‘silicon bragâ€‘gers who want to run this on a MacBook (but still need 80â€¯GB of RAM, soâ€¦ good luck).  
tools/ â€“ toy browsing and Python containers (think â€œChatGPTâ€™s sandbox, but you host itâ€).  
clients/ â€“ terminal chat, Responses API server, and a Codex integration (because why not?).

Installation (or how to lose a weekend)

# Just the tools
pip install gpt-oss

# Torch implementation (requires a GPU farm)
pip install "gpt-oss[torch]"

# Triton implementation (requires you to compile Triton from source)
pip install "gpt-oss[triton]"
Note: The metal build needs GPTOSS_BUILD_METAL=1. If youâ€™re on Linux, ignore this and feel superior.

## Download the Weights (the real work)

# 120â€¯B
hf download openai/gpt-oss-120b --include "original/*" --local-dir gpt-oss-120b/

# 20â€¯B
hf download openai/gpt-oss-20b --include "original/*" --local-dir gpt-oss-20b/
Warning: The download size is roughly â€œa small planetâ€. Make sure you have enough bandwidth and a spare hard drive.

## Running the Reference PyTorch Model (if you love pain)

torchrun --nproc-per-node=4 -m gpt_oss.generate gpt-oss-120b/original/
If you see â€œCUDA out of memoryâ€, congratulationsâ€”youâ€™ve just discovered the GPUâ€‘memory limit of your cluster.

## Running the Tritonâ€‘Optimized Version (singleâ€‘GPU, singleâ€‘night)

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
python -m gpt_oss.generate --backend triton gpt-oss-120b/original/
Still OOM? Turn on the expandable allocator and add more coffee.

## Metal (Appleâ€‘Silicon) â€“ â€œBecause we canâ€

GPTOSS_BUILD_METAL=1 pip install -e ".[metal]"
python gpt_oss/metal/scripts/create-local-model.py -s <model_dir> -d model.bin
python gpt_oss/metal/examples/generate.py model.bin -p "Why is the sky blue?"
Harmony Format & Tools (the â€œsecret sauceâ€)
The Harmony library is the only thing that makes the model actually understand you. Think of it as a very polite but extremely verbose translator.

```python
from openai_harmony import load_harmony_encoding, HarmonyEncodingName

enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
```
Tool examples (browser & Python) are in gpt_oss/tools/. Theyâ€™re stateless (good for demos, terrible for production).

Satirical Disclaimer
We are not responsible if the model decides to write a manifesto about the meaning of life.
Do not use this in any missionâ€‘critical system unless you enjoy catastrophic failure.
Remember: The modelâ€™s reasoning effort is just a fancy knob that changes how long you wait for it to hallucinate.

Citation (for the academic hoarders)

```bibtex
@misc{openai2025gptoss,
  title   = {gpt-oss-120b \& gpt-oss-20b Model Card},
  author  = {OpenAI},
  year    = {2025},
  eprint  = {2508.10925},
  url     = {https://arxiv.org/abs/2508.10925},
}
```

Enjoy the ride, bring a spare GPU, and may your GPU fans never stop spinning! ğŸ¢
